{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d883ee2c",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "If you do not have these modules installed, you will need to install them at the command line, using a BASH shell, Terminal, or the Anaconda Command Prompt.\n",
    "\n",
    "- `wordcloud`\n",
    "- `matplotlib`\n",
    "- `spaCy`\n",
    "- `nltk`\n",
    "- `gensim`\n",
    "\n",
    "## wordcloud\n",
    "\n",
    "\"A little word cloud generator in Python,\" made by Andreas Mueller\n",
    "\n",
    "\"A little word cloud generator in Python,\" made by Andreas Mueller\n",
    "\n",
    "Documentation:\n",
    "- INSTALL: [\"word_cloud\", GitHub](https://github.com/amueller/word_cloud)\n",
    "- [\"WordCloud for Python documentation\"](https://amueller.github.io/word_cloud/)\n",
    "\n",
    "## matplotlib\n",
    "\n",
    "\"Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python...Matplotlib produces publication-quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shell, web application servers, and various graphical user interface toolkits\"\n",
    "\n",
    "Documentation:\n",
    "- INSTALL: [\"Installation,\" matplotlib documentation](https://matplotlib.org/stable/users/installing.html\n",
    "- [\"Documentation,\" matplotlib](https://matplotlib.org/stable/tutorials/index.html)\n",
    "\n",
    "## spaCy\n",
    "\n",
    "\"spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython\"\n",
    "\n",
    "Documentation:\n",
    "- INSTALL: [\"Install spaCy\", spaCy documentation](https://spacy.io/usage)\n",
    "- [\"GUIDES\", spaCy documentation](https://spacy.io/usage/linguistic-features)\n",
    "\n",
    "## nltk\n",
    "\n",
    "\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\"\n",
    "\n",
    "Documentation:\n",
    "- INSTALL\n",
    "  * [\"Installing NLTK,\" NLTK documentation](https://www.nltk.org/install.html)\n",
    "  * [\"Installing NLTK Data,\" NLTK documentation](http://www.nltk.org/data.html)\n",
    "- [NLTK Documentation](https://www.nltk.org/index.html)\n",
    "\n",
    "For more on NLTK:\n",
    "- Steven Bird, Ewan Klein, and Edward Loper, [*Natural Language Processing With Python: Analyzing Text with the Natural Language Toolkit*](http://www.nltk.org/book/) (O'Reilly, 2009).\n",
    "\n",
    "## gensim\n",
    "\n",
    "\"Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) and painlessly (human-wise) as possible. Gensim is designed to process raw, unstructured digital texts (”plain text”) using unsupervised machine learning algorithms.\"\n",
    "\n",
    "The core concepts of gensim are:\n",
    "- Document: some text.\n",
    "- Corpus: a collection of documents.\n",
    "- Vector: a mathematically convenient representation of a document.\n",
    "- Model: an algorithm for transforming vectors from one representation to another.\n",
    "\n",
    "\n",
    "Documentation:\n",
    "- INSTALL: [\"Installation\", Gensim documentation](https://radimrehurek.com/gensim/intro.html#installation)\n",
    "- [\"Documentation,\" Gensim](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
    "\n",
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3ca31dd",
   "metadata": {},
   "source": [
    "# command prompt code for installing packages using pip\n",
    "\n",
    "pip install wordcloud\n",
    "pip install matplotlib\n",
    "pip install spacy\n",
    "pip install nltk\n",
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdcd1ca",
   "metadata": {},
   "source": [
    "# Generate WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# read text\n",
    "text = open('sample_output_file.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# display image using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a903e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower max font size\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad660237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-matplotlib option\n",
    "image = wordcloud.to_image()\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428f3d1",
   "metadata": {},
   "source": [
    "# Getting Started With Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import spacy\n",
    "\n",
    "# load English tokenizer, tagger, parser, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process document\n",
    "spacy_text = text\n",
    "doc = nlp(spacy_text)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d404a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == 'VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82774637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1dffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find named entities\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing named entities\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018e068",
   "metadata": {},
   "source": [
    "# Getting Started With NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad46a2",
   "metadata": {},
   "source": [
    "We can use `nltk.download()` to download additional nltk components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load additional nltk data\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acd329",
   "metadata": {},
   "source": [
    "The next step with `nltk` is to tokenize the text, which then lets us access other components of `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666bb5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text using nltk\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1498217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag tokens by position\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and extract named entities\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc205d",
   "metadata": {},
   "source": [
    "`NLTK` also includes other options for tokenizing a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992137af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39656a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using WordPunctTokenizer \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize using RegexpTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "regex_words = tokenizer.tokenize(text)\n",
    "\n",
    "regex_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b013f",
   "metadata": {},
   "source": [
    "We can use the most effective tokenizing method for this data in combination with a few other data wrangling steps to output a unique list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# remove punctuation/special characters\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "# remove non-text content\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = [w for w in words if not w in stop_words]\n",
    "\n",
    "# removes words with fewer than 3 characters\n",
    "# words = [word for word in words if len(word) > 3]\n",
    "\n",
    "# output cleaned list of words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2122b48",
   "metadata": {},
   "source": [
    "We can then take that list of words and plot term frequency and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk components\n",
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('webtext')\n",
    "\n",
    "# analyze term frequency/distribution\n",
    "data_analysis = nltk.FreqDist(words)\n",
    "\n",
    "data_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdeab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot term frequency/distribution for all terms\n",
    "data_analysis.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show term frequency/distribution for top 10 terms\n",
    "for word, frequency in data_analysis.most_common(10):\n",
    "    print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d7de2",
   "metadata": {},
   "source": [
    "If you really want to dig in with NLTK:\n",
    "- Steven Bird, Ewan Klein, and Edward Loper, [*Natural Language Processing With Python: Analyzing Text with the Natural Language Toolkit*](http://www.nltk.org/book/) (O'Reilly, 2009)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad1ae8",
   "metadata": {},
   "source": [
    "# Getting Started With Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a gensim dictionary from a single txt file\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "from gensim import corpora\n",
    "import os\n",
    "\n",
    "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('sample.txt'))\n",
    "\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow\n",
    "\n",
    "\n",
    "# Create the Dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# Create the Corpus\n",
    "bow_corpus = BoWCorpus('sample.txt', dictionary=mydict)  # memory friendly\n",
    "\n",
    "# Print the token_id and count for each line.\n",
    "for line in bow_corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show word weights\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "for doc in bow_corpus:\n",
    "    print([[mydict[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words from single text file\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# set up classes for bag of words model\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow\n",
    "\n",
    "\n",
    "# create dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# build corpus from txt file\n",
    "bow_corpus = BoWCorpus('sample.txt', dictionary=mydict)  # memory friendly\n",
    "\n",
    "# print token id and count for each line\n",
    "for line in bow_corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87282cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf model\n",
    "tfidf = models.TfidfModel(bow_corpus, smartirs='ntc')\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "for doc in tfidf[bow_corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede93c7",
   "metadata": {},
   "source": [
    "## Python Text Analysis Next Steps and Additional Resources\n",
    "\n",
    "### More documentation on tools covered in this notebook\n",
    "\n",
    "**`wordcloud`**\n",
    "- wordcloud, \"[WordCloud for Python documentation](https://amueller.github.io/word_cloud/)\"\n",
    "- wordcloud, \"[Gallery of Examples](https://amueller.github.io/word_cloud/auto_examples/index.html#example-gallery)\"\n",
    "- Duong Vu, \"[Generating Word Clouds in Python](https://www.datacamp.com/community/tutorials/wordcloud-python)\" *DataCamp* (8 November 2019)\n",
    "- Zolzaya Luvsandorj, \"[Simple Word Cloud in Python](https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5)\" *Towards Data Science* (20 June 2020)\n",
    "\n",
    "**`spaCy`**\n",
    "- spaCy, \"[spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101)\"\n",
    "- spaCy, \"[Advanced NLP with spaCy](https://course.spacy.io/en/)\"\n",
    "- Ines Montani, \"[spaCy Cheat Sheet: Advanced NLP in Python](https://www.datacamp.com/community/blog/spacy-cheatsheet)\" *DataCamp* (14 July 2021)\n",
    "- Conor Mc., \"[A short introduction to NLP in Python with spaCy](https://towardsdatascience.com/a-short-introduction-to-nlp-in-python-with-spacy-d0aa819af3ad)\" *Towards Data Science* (17 March 2017)\n",
    "\n",
    "**`nltk`**\n",
    "- Steven Bird, Ewan Klein, and Edward Loper, *[Natural Language Processing With Python: Analyzing Text with the Natural Language Toolkit](http://www.nltk.org/book/)* (O'Reilly, 2009).\n",
    "  * Free online book includes chapters on processing tasks, categorizing/tagging words, classifying text, extracting information, analyzing sentence structure, and other tasks.\n",
    "- Avinash Navlani, \"[Text Analytics for Beginners using NLTK](https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk)\" *DataCamp* (13 December 2019)\n",
    "- Michelle Morales, \"[How to Work with Language Data in Python 3 Using the Natural Language Toolkit (NLTK)](https://www.digitalocean.com/community/tutorials/how-to-work-with-language-data-in-python-3-using-the-natural-language-toolkit-nltk)\" *Digital Ocean* (3 January 2017)\n",
    "- Shaumik Daityari, \"[How to Perform Sentiment Analysis in Python 3 using the Natural Langauge Toolkit (NLTK)](https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk)\" *Digital Ocean* (26 September 2019)\n",
    "- Zoë Wilkinson Saldaña, \"Sentiment Analysis for Exploratory Data Analysis,\" The Programming Historian 7 (2018), https://doi.org/10.46430/phen0079.\n",
    "- François Dominic Laramée, \"Introduction to stylometry with Python,\" The Programming Historian 7 (2018), https://doi.org/10.46430/phen0078.\n",
    "\n",
    "### Other Tutorials, Packages, and Methods\n",
    "\n",
    "For a more detailed overview of text analysis methods- START HERE before diving into a new method/tutorial:\n",
    "- Heather Froelich, \"[Text Analysis: An Introduction, Methods](https://guides.libraries.psu.edu/c.php?g=829065&p=5922906)\" *PennState University Library Guide*\n",
    "- Stéfan Sinclair & Geoffrey Rockwell, *[The Art of Literary Text Analysis](https://nbviewer.jupyter.org/github/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb)*, edited by Melissa Mony (last modified 12 January 2018)\n",
    "  * Includes Jupyter notebooks with more resources on `nltk`, visualizing textual data, and a variety of analysis methods \n",
    "\n",
    "For a more detailed overview of various tools and tutorials (beyond those listed here):\n",
    "- Alan Liu, \"[Tutorials for DH Tools and Methods, Text Analysis](http://dhresourcesforprojectbuilding.pbworks.com/w/page/69244314/Tutorials%20for%20DH%20Tools%20and%20Methods#tutorials-text-analysis)\" *digital humanities resources for project building* (2019)\n",
    "\n",
    "**List of words and word frequency**\n",
    "- William J. Turkel and Adam Crymble, \"Normalizing Textual Data with Python,\" The Programming Historian 1 (2012), https://doi.org/10.46430/phen0014.\n",
    "- William J. Turkel and Adam Crymble, \"Counting Word Frequencies with Python,\" The Programming Historian 1 (2012), https://doi.org/10.46430/phen0003.\n",
    "- William J. Turkel and Adam Crymble, \"Keywords in Context (Using n-grams) with Python,\" The Programming Historian 1 (2012), https://doi.org/10.46430/phen0010.\n",
    "\n",
    "**TF-IDF (term frequency, inverse document frequency)**\n",
    "- Matthew J. Lavin, \"Analyzing Documents with TF-IDF,\" The Programming Historian 8 (2019), https://doi.org/10.46430/phen0082.\n",
    "- John R. Ladd, \"Understanding and Using Common Similarity Measures for Text Analysis,\" The Programming Historian 9 (2020), https://doi.org/10.46430/phen0089.\n",
    "  * Both of these tutorials use [`scikit-learn`'s tf-idf implementation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "**Topic Modeling (looking for patterns of words in text/corpus to build topics, builds on td-idf)**\n",
    "- For more specifics on topic modeling as a methodology: \n",
    "  * Ted Underwood, \"[Topic modeling made just simple enough](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/) *blog* (7 April 2012)\n",
    "  * Scott Weingart, \"[Topic Modeling for Humanists: A Guided Tour](http://scottbot.net/topic-modeling-for-humanists-a-guided-tour/)\" *blog* (25 July 2012)\n",
    "- Shawn Graham, Scott Weingart, and Ian Milligan, \"Getting Started with Topic Modeling and MALLET,\" The Programming Historian 1 (2012), https://doi.org/10.46430/phen0017.\n",
    "- Shanshank Kapadia, \"[Topic Modeling in Python: Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)\" *Towards Data Science* (14 April 2019)\n",
    "- Susan Li, \"[Topic Modelling in Python with NLTK and Gensim](https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21)\" *Towards Data Science* (30 March 2018)\n",
    "\n",
    "**Sentiment analysis (emotional intensity/weight for words and phrases in a text)**\n",
    "- Zoë Wilkinson Saldaña, \"Sentiment Analysis for Exploratory Data Analysis,\" The Programming Historian 7 (2018), https://doi.org/10.46430/phen0079.\n",
    "\n",
    "**Stylometry (literary style, authorship attribution)**\n",
    "- François Dominic Laramée, \"Introduction to stylometry with Python,\" The Programming Historian 7 (2018), https://doi.org/10.46430/phen0078.\n",
    "\n",
    "**Geoparsing (extracting and plotting location information)**\n",
    "- Beatrice Alex, \"Geoparsing English-Language Text with the Edinburgh Geoparser,\" The Programming Historian 6 (2017), https://doi.org/10.46430/phen0067.\n",
    "\n",
    "**Working with other source material (other archives, APIs, etc)**\n",
    "- Stephen Krewson, \"Extracting Illustrated Pages from Digital Libraries with Python,\" The Programming Historian 8 (2019), https://doi.org/10.46430/phen0084.\n",
    "  *  Tutorial covers workflows for getting digital image content from HathiTrust and Internet Archive, but could also get you started with an OCR workflow for texts in these collections.\n",
    "- Resources for using the Library of Congress' \"Chronicling America\" collection (wide variety of national and local U.S. newspapers, 1777-1963)\n",
    "  * Library of Congress, \"[About the Site and API](https://chroniclingamerica.loc.gov/about/api/)\" *Chronicling America: Historic American Newspapers*\n",
    "  * Library of Congress, \"[Available Tutorials](https://libraryofcongress.github.io/data-exploration/all-tutorials.html)\" *loc.gov JSON API*\n",
    "  * Hugo van Kemenade, \"[chroniclingamerica.py: Python API to search Chronicling America newspaper pages](https://github.com/hugovk/chroniclingamerica.py)\" *GitHub* (2016)\n",
    "  * Jason Heppler, \"[Working with the Chronicling America API](https://observablehq.com/@hepplerj/working-with-the-chronicling-america-api)\" *Observable* (6 July 2020)\n",
    "\n",
    "**Stanford Natural Language Processing Group**\n",
    "\n",
    "From their \"[Software](https://nlp.stanford.edu/software/)\" page: \"The Stanford NLP Group makes some of our Natural Language Processing software available to everyone! We provide statistical NLP, deep learning NLP, and rule-based NLP tools for major computational linguistics problems, which can be incorporated into applications with human language technology needs. These packages are widely used in industry, academia, and government...All our supported software distributions are written in Java.\"\n",
    "\n",
    "**`scikit-learn`**\n",
    "- `scikit-learn`, \"[Working With Text Data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\": The `scikit-learn` machine-larning library includes a nubmer of modules and resources for natural language processing, including tasks like feature extraction and linear models for categorization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
