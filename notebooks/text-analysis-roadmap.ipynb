{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d883ee2c",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "If you do not have these modules installed, you will need to install them at the command line, using a BASH shell, Terminal, or the Anaconda Command Prompt.\n",
    "\n",
    "- `wordcloud`\n",
    "- `matplotlib`\n",
    "- `spaCy`\n",
    "- `nltk`\n",
    "- `gensim`\n",
    "\n",
    "## wordcloud\n",
    "\n",
    "\"A little word cloud generator in Python,\" made by Andreas Mueller\n",
    "\n",
    "\"A little word cloud generator in Python,\" made by Andreas Mueller\n",
    "\n",
    "Documentation:\n",
    "- INSTALL: [\"word_cloud\", GitHub](https://github.com/amueller/word_cloud)\n",
    "- [\"WordCloud for Python documentation\"](https://amueller.github.io/word_cloud/)\n",
    "\n",
    "## matplotlib\n",
    "\n",
    "\"Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python...Matplotlib produces publication-quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shell, web application servers, and various graphical user interface toolkits\"\n",
    "\n",
    "Documentation:\n",
    "- INSTALL: [\"Installation,\" matplotlib documentation](https://matplotlib.org/stable/users/installing.html\n",
    "- [\"Documentation,\" matplotlib](https://matplotlib.org/stable/tutorials/index.html)\n",
    "\n",
    "## spaCy\n",
    "\n",
    "\"spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython\"\n",
    "\n",
    "Documentation:\n",
    "- INSTALL: [\"Install spaCy\", spaCy documentation](https://spacy.io/usage)\n",
    "- [\"GUIDES\", spaCy documentation](https://spacy.io/usage/linguistic-features)\n",
    "\n",
    "## nltk\n",
    "\n",
    "\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\"\n",
    "\n",
    "Documentation:\n",
    "- INSTALL\n",
    "  * [\"Installing NLTK,\" NLTK documentation](https://www.nltk.org/install.html)\n",
    "  * [\"Installing NLTK Data,\" NLTK documentation](http://www.nltk.org/data.html)\n",
    "- [NLTK Documentation](https://www.nltk.org/index.html)\n",
    "\n",
    "For more on NLTK:\n",
    "- Steven Bird, Ewan Klein, and Edward Loper, [*Natural Language Processing With Python: Analyzing Text with the Natural Language Toolkit*](http://www.nltk.org/book/) (O'Reilly, 2009).\n",
    "\n",
    "## gensim\n",
    "\n",
    "\"Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) and painlessly (human-wise) as possible. Gensim is designed to process raw, unstructured digital texts (”plain text”) using unsupervised machine learning algorithms.\"\n",
    "\n",
    "The core concepts of gensim are:\n",
    "- Document: some text.\n",
    "- Corpus: a collection of documents.\n",
    "- Vector: a mathematically convenient representation of a document.\n",
    "- Model: an algorithm for transforming vectors from one representation to another.\n",
    "\n",
    "\n",
    "Documentation:\n",
    "- INSTALL: [\"Installation\", Gensim documentation](https://radimrehurek.com/gensim/intro.html#installation)\n",
    "- [\"Documentation,\" Gensim](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
    "\n",
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3ca31dd",
   "metadata": {},
   "source": [
    "# command prompt code for installing packages using pip\n",
    "\n",
    "pip install wordcloud\n",
    "pip install matplotlib\n",
    "pip install spacy\n",
    "pip install nltk\n",
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdcd1ca",
   "metadata": {},
   "source": [
    "# Generate WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# read text\n",
    "text = open('sample_output_file.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# display image using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a903e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower max font size\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad660237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-matplotlib option\n",
    "image = wordcloud.to_image()\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428f3d1",
   "metadata": {},
   "source": [
    "# Getting Started With Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import spacy\n",
    "\n",
    "# load English tokenizer, tagger, parser, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process document\n",
    "spacy_text = text\n",
    "doc = nlp(spacy_text)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d404a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == 'VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82774637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1dffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find named entities\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing named entities\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018e068",
   "metadata": {},
   "source": [
    "# Getting Started With NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad46a2",
   "metadata": {},
   "source": [
    "We can use `nltk.download()` to download additional nltk components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load additional nltk data\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acd329",
   "metadata": {},
   "source": [
    "The next step with `nltk` is to tokenize the text, which then lets us access other components of `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666bb5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text using nltk\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1498217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag tokens by position\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and extract named entities\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc205d",
   "metadata": {},
   "source": [
    "`NLTK` also includes other options for tokenizing a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992137af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39656a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using WordPunctTokenizer \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize using RegexpTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "regex_words = tokenizer.tokenize(text)\n",
    "\n",
    "regex_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b013f",
   "metadata": {},
   "source": [
    "We can use the most effective tokenizing method for this data in combination with a few other data wrangling steps to output a unique list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# remove punctuation/special characters\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "# remove non-text content\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = [w for w in words if not w in stop_words]\n",
    "\n",
    "# removes words with fewer than 3 characters\n",
    "# words = [word for word in words if len(word) > 3]\n",
    "\n",
    "# output cleaned list of words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2122b48",
   "metadata": {},
   "source": [
    "We can then take that list of words and plot term frequency and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk components\n",
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('webtext')\n",
    "\n",
    "# analyze term frequency/distribution\n",
    "data_analysis = nltk.FreqDist(words)\n",
    "\n",
    "data_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdeab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot term frequency/distribution for all terms\n",
    "data_analysis.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show term frequency/distribution for top 10 terms\n",
    "for word, frequency in data_analysis.most_common(10):\n",
    "    print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d7de2",
   "metadata": {},
   "source": [
    "If you really want to dig in with NLTK:\n",
    "- Steven Bird, Ewan Klein, and Edward Loper, [*Natural Language Processing With Python: Analyzing Text with the Natural Language Toolkit*](http://www.nltk.org/book/) (O'Reilly, 2009)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad1ae8",
   "metadata": {},
   "source": [
    "# Getting Started With Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a gensim dictionary from a single txt file\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "from gensim import corpora\n",
    "import os\n",
    "\n",
    "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('sample.txt'))\n",
    "\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow\n",
    "\n",
    "\n",
    "# Create the Dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# Create the Corpus\n",
    "bow_corpus = BoWCorpus('sample.txt', dictionary=mydict)  # memory friendly\n",
    "\n",
    "# Print the token_id and count for each line.\n",
    "for line in bow_corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show word weights\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "for doc in bow_corpus:\n",
    "    print([[mydict[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words from single text file\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# set up classes for bag of words model\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow\n",
    "\n",
    "\n",
    "# create dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# build corpus from txt file\n",
    "bow_corpus = BoWCorpus('sample.txt', dictionary=mydict)  # memory friendly\n",
    "\n",
    "# print token id and count for each line\n",
    "for line in bow_corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87282cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf model\n",
    "tfidf = models.TfidfModel(bow_corpus, smartirs='ntc')\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "for doc in tfidf[bow_corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
