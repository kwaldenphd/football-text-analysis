{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Notre Dame Observer PDFs\n",
    "\n",
    "From the [University Archives](http://archives.nd.edu/digital/):\n",
    "- \"The Observer started providing news for the University of Notre Dame and Saint Mary's College starting in the fall of 1966, first as a weekly, then bi-weekly, and soon as a daily newspaper. Starting with the issue of October 10, 2009, the Notre Dame / Saint Mary's Observer appeared online (http://ndsmcobserver.com/).\"\n",
    "- [The Observer (student newspaper), 1966 - 2015](http://archives.nd.edu/Observer/)\n",
    "\n",
    "This Jupyter Notebook inclues codes + comments that downloads all *Observer* PDFs, and also matches issue titles to file names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries, Load URL, and Create Beautiful Soup Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load url, create beautifulsoup object\n",
    "page = requests.get('http://archives.nd.edu/Observer/Observer.htm')\n",
    "\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# isolate HTML with 'ol' tag\n",
    "url_names = soup.find('ul')\n",
    "\n",
    "# find all instances of 'a' tag\n",
    "items = url_names.find_all('a')\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get List of Volume Links and Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list for volume urls\n",
    "vol_url_list = []\n",
    "\n",
    "# create empty list for volume titles\n",
    "vol_title_list = []\n",
    "\n",
    "# for loop that extracts href contents, concatenates full url, appends to url_list; extracts tag contents (volume title) and appends to title_list\n",
    "for item in items:\n",
    "    vol_url_list.append(\"http://archives.nd.edu/Observer/\" + item.get('href'))\n",
    "    vol_title_list.append(item.contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample volume url\n",
    "vol_url_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample volume title\n",
    "vol_title_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get List of Issue Links and Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list for issue html elements\n",
    "issue_items = []\n",
    "\n",
    "# create empty list for a tags\n",
    "a_tags = []\n",
    "\n",
    "\n",
    "# for loop that loads each volume page as beautifulsoup object, extracts 'a' tag elements on each page, appends to issue_items list\n",
    "for url in vol_url_list:\n",
    "    try:\n",
    "        single_page = requests.get(url)\n",
    "        soup = BeautifulSoup(single_page.text, 'html.parser')\n",
    "        a_tags.append(soup.find('ol'))\n",
    "        for tag in a_tags:\n",
    "            url_names = tag.find_all('li')\n",
    "            issue_items.append(url_names)           \n",
    "            \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample issue_items value\n",
    "issue_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list for issue titles\n",
    "issue_titles = []\n",
    "\n",
    "for issue in issue_items:\n",
    "    for i in issue:\n",
    "        issue_title = i.contents[0]\n",
    "        issue_date = i.contents[1].string.strip()\n",
    "        issue_titles.append(issue_title + issue_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert issue_titles list to set and back to return only unique values\n",
    "issue_titles = list(set(issue_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample issue title\n",
    "issue_titles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Beginning in Volume 45 (2010-2011 academic year), the University Archive digital collections does not include separate black/white and color editions of each issue. This modified structure does not affect the code extracting issue names but does require modified code to extract issue URLs and file names. The `bw_color_issue_links` includes content for Volume 45 onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list for black and white issue links\n",
    "bw_issue_links = []\n",
    "\n",
    "# empty list for color issue links\n",
    "color_issue_links = []\n",
    "\n",
    "# empty list for bw/color issue links\n",
    "bw_color_issue_links = []\n",
    "\n",
    "# import re\n",
    "import re\n",
    "\n",
    "# for loop that extracts href content for each issue version and appends to respective list\n",
    "for issue in issue_items:\n",
    "        for i in issue:\n",
    "            try:\n",
    "                bw_url = i.contents[3].get('href')\n",
    "                bw_url = re.sub(\"^../\",\"\",bw_url)\n",
    "                bw_url = re.sub(\"^../\",\"\",bw_url)\n",
    "                bw_issue_links.append(bw_url)\n",
    "                color_url = i.contents[5].get('href')\n",
    "                color_issue_links.append(re.sub(\"^../\",\"\", color_url))\n",
    "            except:\n",
    "                bw_color = i.contents[3].get('href')\n",
    "                bw_color_issue_links.append(re.sub(\"^../\",\"\", bw_color))\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to sets and back again to return only unique values\n",
    "bw_issue_links = list(set(bw_issue_links))\n",
    "\n",
    "color_issue_links = list(set(color_issue_links))\n",
    "\n",
    "bw_color_issue_links = list(set(bw_color_issue_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample black/white issue link\n",
    "bw_issue_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample color issue link\n",
    "color_issue_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample bw/color issue link\n",
    "bw_color_issue_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Full URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list for concatenated black/white issue urls\n",
    "full_bw_issue_links = []\n",
    "\n",
    "for link in bw_issue_links:\n",
    "    v45 = re.findall('v45', link)\n",
    "    v46 = re.findall('v46', link)\n",
    "    v47 = re.findall('v47', link)\n",
    "    v48 = re.findall('v48', link)\n",
    "    v49 = re.findall('v49', link)\n",
    "    v50 = re.findall('v50', link)\n",
    "    if v45:\n",
    "        full_bw_issue_links.append(\"http://archives.nd.edu/Observer/v45/\" + link)\n",
    "    elif v46:\n",
    "        full_bw_issue_links.append(\"http://archives.nd.edu/Observer/v46/\" + link)\n",
    "    elif v47:\n",
    "        full_bw_issue_links.append(\"http://archives.nd.edu/Observer/v47/\" + link)\n",
    "    elif v48:\n",
    "        full_bw_issue_links.append(\"http://archives.nd.edu/Observer/v48/\" + link)\n",
    "    elif v49:\n",
    "        full_bw_issue_links.append(\"http://archives.nd.edu/Observer/v49/\" + link)\n",
    "    elif v50:\n",
    "        full_bw_issue_links.append(\"http://archives.nd.edu/Observer/v50/\" + link)\n",
    "        \n",
    "# show sample black/white full issue link\n",
    "full_bw_issue_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list for concatenated issue urls\n",
    "full_bw_color_issue_links = []\n",
    "\n",
    "for link in bw_color_issue_links:\n",
    "    v45 = re.findall('v45', link)\n",
    "    v46 = re.findall('v46', link)\n",
    "    v47 = re.findall('v47', link)\n",
    "    v48 = re.findall('v48', link)\n",
    "    v49 = re.findall('v49', link)\n",
    "    v50 = re.findall('v50', link)\n",
    "    if v45:\n",
    "        full_bw_color_issue_links.append(\"http://archives.nd.edu/Observer/v45/\" + link)\n",
    "    elif v46:\n",
    "        full_bw_color_issue_links.append(\"http://archives.nd.edu/Observer/v46/\" + link)\n",
    "    elif v47:\n",
    "        full_bw_color_issue_links.append(\"http://archives.nd.edu/Observer/v47/\" + link)\n",
    "    elif v48:\n",
    "        full_bw_color_issue_links.append(\"http://archives.nd.edu/Observer/v48/\" + link)\n",
    "    elif v49:\n",
    "        full_bw_color_issue_links.append(\"http://archives.nd.edu/Observer/v49/\" + link)\n",
    "    elif v50:\n",
    "        full_bw_color_issue_links.append(\"http://archives.nd.edu/Observer/v50/\" + link)\n",
    "        \n",
    "# show sample full issue link\n",
    "full_bw_color_issue_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download PDFs from List of Full URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Black/White Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure urllib\n",
    "http = urllib3.PoolManager()\n",
    "print(\"downloading with urllib\")\n",
    "\n",
    "# for loop that downloads PDF for each url in full_bw_issue_links\n",
    "for link in full_bw_issue_links:\n",
    "    r = http.request('GET', link)\n",
    "    filename = os.path.basename(link)\n",
    "    with open (filename, 'wb') as fcont:\n",
    "        fcont.write(r.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Color Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure urllib\n",
    "http = urllib3.PoolManager()\n",
    "print(\"downloading with urllib\")\n",
    "\n",
    "# for loop that downloads PDF for each url in color_issue_links\n",
    "for link in color_issue_links:\n",
    "    r = http.request('GET', link)\n",
    "    filename = os.path.basename(link)\n",
    "    with open (filename, 'wb') as fcont:\n",
    "        fcont.write(r.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Issues for Volumes 45-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure urllib\n",
    "http = urllib3.PoolManager()\n",
    "print(\"downloading with urllib\")\n",
    "\n",
    "# for loop that downloads PDF for each url in full_bw_color_issue_links\n",
    "for link in full_bw_color_issue_links:\n",
    "    r = http.request('GET', link)\n",
    "    filename = os.path.basename(link)\n",
    "    with open (filename, 'wb') as fcont:\n",
    "        fcont.write(r.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching File Names and Volume/Issue Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list for black and white issue file names\n",
    "bw_issue_names = []\n",
    "\n",
    "# empty list for black and white issue titles\n",
    "bw_issue_titles = []\n",
    "\n",
    "# empty list for color issue file names\n",
    "color_issue_names = []\n",
    "\n",
    "# empty list for color issue titles\n",
    "color_issue_titles = []\n",
    "\n",
    "# empty list for bw/color issue file names\n",
    "bw_color_issue_names = []\n",
    "\n",
    "# empty list for bw/color issue titles\n",
    "bw_color_issue_titles = []\n",
    "\n",
    "# import re\n",
    "import re\n",
    "\n",
    "# for loop that extracts file name and title each issue version and appends to respective list\n",
    "for issue in issue_items:\n",
    "        for i in issue:\n",
    "            try:\n",
    "                bw_url = i.contents[3].get('href')\n",
    "                bw_url = re.sub(\"^../\",\"\",bw_url)\n",
    "                bw_url = re.sub(\"^../\",\"\",bw_url)\n",
    "                bw_issue_names.append(bw_url)\n",
    "                bw_issue_title = i.contents[0]\n",
    "                bw_issue_date = i.contents[1].string.strip()\n",
    "                bw_issue = bw_issue_title + bw_issue_date\n",
    "                bw_issue_titles.append(bw_issue)\n",
    "                color_url = i.contents[5].get('href')\n",
    "                color_issue_names.append(re.sub(\"^../\",\"\", color_url))\n",
    "                color_issue_title = i.contents[0]\n",
    "                color_issue_date = i.contents[1].string.strip()\n",
    "                color_issue = color_issue_title + color_issue_date\n",
    "                color_issue_titles.append(color_issue) \n",
    "            except:\n",
    "                bw_color = i.contents[3].get('href')\n",
    "                bw_color_issue_names.append(re.sub(\"^../\",\"\", bw_color))\n",
    "                bw_color_issue_title = i.contents[0]\n",
    "                bw_color_issue_date = i.contents[1].string.strip()\n",
    "                bw_color_issue = bw_color_issue_title + bw_color_issue_date\n",
    "                bw_color_issue_titles.append(bw_color_issue)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black/White Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# create empty dataframe with two columns\n",
    "bwdf = pd.DataFrame(columns=['file_name', 'title'])\n",
    "\n",
    "# append issue_url_list to file_name column\n",
    "bwdf['file_name'] = bw_issue_names\n",
    "\n",
    "# append full_issue_title values to title column\n",
    "bwdf['title'] = bw_issue_titles\n",
    "\n",
    "# remove duplicates\n",
    "bwdf = bwdf.drop_duplicates()\n",
    "\n",
    "# show updated dataframe\n",
    "bwdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write bwdf dataframe to csv file\n",
    "bwdf.to_csv(\"observer_bw_file_name_master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# create empty dataframe with two columns\n",
    "cdf = pd.DataFrame(columns=['file_name', 'title'])\n",
    "\n",
    "# append issue_url_list to file_name column\n",
    "cdf['file_name'] = color_issue_names\n",
    "\n",
    "# append full_issue_title values to title column\n",
    "cdf['title'] = color_issue_titles\n",
    "\n",
    "# remove duplicates\n",
    "cdf = cdf.drop_duplicates()\n",
    "\n",
    "# show updated dataframe\n",
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cdf dataframe to csv file\n",
    "cdf.to_csv(\"observer_color_file_name_master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volumes 45-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# create empty dataframe with two columns\n",
    "df = pd.DataFrame(columns=['file_name', 'title'])\n",
    "\n",
    "# append issue_url_list to file_name column\n",
    "df['file_name'] = bw_color_issue_names\n",
    "\n",
    "# append full_issue_title values to title column\n",
    "df['title'] = bw_color_issue_titles\n",
    "\n",
    "# remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# show updated dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "df.to_csv(\"observer_bw_color_file_name_master.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
